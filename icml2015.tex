\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
%
\mainmatter              % start of the contributions
%
\title{A Machine Learning Application that Matters}
%
\titlerunning{ML Application that Matters}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Geoff Holmes\inst{1} \and Dale Fletcher\inst{2} \and Peter Reutemann\inst{3} \and Martijn van Oostrum\inst{4}}
%
\authorrunning{Holmes et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Geoff Holmes, Dale Fletcher, Peter Reutemann}
%
\institute{
University of Waikato, Hamilton, NZ, \email{geoff@waikato.ac.nz}
\and
University of Waikato, Hamilton, NZ, \email{dale@waikato.ac.nz}
\and
University of Waikato, Hamilton, NZ, \email{fracpete@waikato.ac.nz}
\and
BLGG AgroXpertus, Wageningen, NL, \email{martijn.vanoostrum@blgg.agroxpertus.com}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
TODO
\end{abstract}

%%%%%%%%%%%%%%%%
% Introduction %
%%%%%%%%%%%%%%%%
\section{Introduction}
\begin{itemize}
  \item Machine learning that matters?? See Wagstaff’s paper (citations) \\
    http://scholar.google.com/scholar?oi=bibs\&hl=en\&cites=2774690908240883628
  \item Martijn - business side
\end{itemize}

\subsection{BLGG AgroXpertus}

BLGG AgroXpertus, founded in 1928, is based in Wageningen, the Netherlands, and has 200 employees. The company is part of the BLGG Group, which focuses on various types of analysis. BLGG AgroXpertus focuses on agricultural analyses. The company analyses some 500,000 samples of soil, roughage, manure, compost, nutrient solutions and crops. 
'Wageningen' is a worldwide name in the fields of leading agricultural knowledge and innovations. 

\subsection{Efficiency improvement for farmers}

The mission of BLGG AgroXpertus is to improve the efficiency of individual farmers. This is possible with innovative techniques that lead to new analytic and agricultural knowledge. 

In over 80 years BLGG AgroXpertus developed a wealth of knowledge and skills in professional sampling and analysis of soil, but also of roughage, compost and manure. 

BLGG AgroXpertus focuses on the areas: 
\begin{itemize}
  \item Soil nutrition and crop fertilization 
  \item Soil Health 
  \item Plant Health 
  \item Roughage analysis 
\end{itemize}

For the markets: 
\begin{itemize}
  \item Horticultural farming 
  \item Arable farming 
  \item Dairy farming 
  \item Greenhouse growing 
\end{itemize}

Due to its knowledge BLGG AgroXpertus can provide farmers worldwide with additional information, which they use to improve the yield and quality of their agricultural production, and reduce costs. 


\subsection{last year}

\begin{itemize}
  \item forage 140000 samples with 115000 in wageningen and 25000 in samplinq 
  \item if you would have done all of these through ref analyses at 200 euro a samples total saving will be 28 million euros a year 
  \item soil  on average 85000 samples a year also at least 200 euro when doing all analyses though wet chemistry so 17 million euros a year.. 
  \item in a derogation season soil 150000 samples $\rightarrow$ 30 million in savings a year 
  \item but forage is known for nir  predictive analyses, soil is not 
\end{itemize}


%%%%%%%%%
% ADAMS %
%%%%%%%%%
\section{ADAMS}
ADAMS, the Advanced Data mining and Machine learning System, is a modular,
scientifc workflow engine written in Java. Currently available modules
include support for Weka, MOA, R, image processing (ImageJ, JAI, ImageMagick,
Gnuplot), PDF management and display, spreadsheet manipulation (CSV,
Gnumeric, Excel, ODF), scripting (Groovy, Jython), GIS support
(OpenStreetMap), Twitter, time-series analysis, network support (SSH,
SCP, SFTP/FTP, Email), XML/HTML/JSON processing facilities and
webservice capabilities.

In contrast to current versions of other workflow engines, like Kepler, KNIME
or RapidMiner, where the user places operators on a canvas and connects them
manually, ADAMS organizes the operators (or “actors”) of a workflow
automatically in a tree structure without explicit connections. Instead, so
called “control” actors determine how data flows between actors. Examples of
control actors are, e.g., Sequence, Branch, Tee, Trigger, IfThenElse and
Switch. Apart from “flow control”, there are two further aspects to an actor:
“functional” (primitive actor or manages nested actors) and “procedural” in
terms of input/output of data (standalone, source, transformer, sink).

Using a tree layout has advantages and disadvantages. In terms of advantages,
the layout is very compact, it scales to thousands of actors, avoids
having to manually rearrange the workflow in order to include additional
actors (disconnect/reconnect), is context aware when adding actors (data
types of input and output limit what actors can be inserted) and has
customizable rules for suggesting actors depending on context for common
sequences of actors. Disadvantages are, the tree layout is less intuitive
compared to a canvas-based approach and it only supports 1-to-n connections.
The 1-to-n limitation is mitigated using call-able actors (multiple actors can
channel data into a single actor using its name), containers for storing
multiple outputs, variables and internal storage (re-using data in multiple
locations). Variables can be either used in expressions, e.g., ones for
evaluating mathematical formulas, or attached to parameters of operators. The
latter allows for influencing the flow execution, e.g., for the turning on/off
of sub-flows or dynamically changing the setup of a learning algorithm. The
scope of variables and internal storage can be limited using the LocalScope
control actor.

Some further feature highlights: Though ADAMS is a data-driven workflow,
transporting the data in so called “tokens”, by design rather than an
event-based one, i.e., actors get executed if there is data available for them
to process, it is also possible to trigger sub-flows using cronjobs.  This
allows, for instance, for recurring clean-up operations. Interactive actors are
very useful for developing workflow applications. These actors either prompt
the user to enter or select a value, controlling sub-flow execution, or require
the user to inspect data, e.g., visual inspection of images, influencing the
data flow. By supporting scripting (Groovy/Jython), it is possible to quickly
prototype new actors without the need of compiling Java code and restarting the
workflow application.  Once a workflow has been developed, it is not necessary
to use the graphical user interface for executing it, the command-line can be
used as well (e.g., for use in a headless server environment). Java-code
generation from existing flows is possible as well.

TODO screenshots, simple example


%%%%%%%%%%%%%%%
% AgroXpertus %
%%%%%%%%%%%%%%%
\section{AgroXpertus}
ADAMS - meta-flows, S2000

Features of NIR-focused ADAMS
\begin{itemize}
  \item DataFusion: NIR and results from other tests
  \item Preprocessing
  \item Evaluation
  \item Outlier detection
  \item (automatic) retraining
  \item Weka (modelling) $\rightarrow$ deployment in business process
  \item before deployment: test multiple models in parallel on real dataset
  \item easy to develop (and deploy) new algorithms using Weka (open-source plugin framework)
  \item meta-flows: you only configure processing/prediction, actual worker flows get generated on the fly
\end{itemize}


%%%%%%%%%%%%%%
% Discussion %
%%%%%%%%%%%%%%
\section{Discussion}
\begin{itemize}
  \item Cost-benefit analysis (Martijn)
  \item Number of predictions (Martijn)
  \item Percentages going to wet chemistry (Martijn)
  \item Six Impact Challenges: \\
    2. \$100M saved through improved decision making provided by an ML system $\rightarrow$ S2000
  \item Lack of Follow-through \\
	ADAMS $\rightarrow$ easily integrate ML system into business processes (opposed to plain Weka)

\subsection{NIR}
\subsubsection{Introduction}
The regression method of choice for most practitioners working with near infrared spectroscopy (NIR) datasets is PLS regression. This method is fast, accurate, straightforward to implement and requires the tuning of a single parameter to maximize performance. Typically it is applied to relatively small datasets, those with fewer than 50 samples. In this study we introduce some known and some novel methods for handling NIR data and compare their performance against PLS regression for a range of NIR datasets of differing size.

\subsubsection{Materials and Methods}
All data in this study originates from a FOSS 5000 instrument. The raw data is down sampled to 170 values per sample and smoothed using a Savitzky-Golay filter with a window of size 15. Two validation studies are undertaken both employing 10x10 cross-validation. The estimated RMSEP, is obtained as an average over 100 runs. In each run 90\% of the data is used for training a model and 10\% is held-out for testing. The corrected re-sampled t-test is then used to perform pair wise comparison between methods testing for significant differences. The first study varies the number of components (from 5 to 65) for PLS regression (PLSR) in order to find the optimum performance for that method on each dataset. The second study then carries forward these results and compares them with some well-known and novel regression methods. These other methods use sensible default, rather than optimized values for their parameters. The methods compared with PLSR are locally weighted PLS regression using 20 PLS components (LWPR), 
a variant of locally weighted learning that uses the PLS components to find nearest neighbors and selects the original data to build a linear regression model (LWPL), a random regression forest using PLS transformed data using 20 components (RRFP), and a number of techniques using untransformed data namely, Gaussian Processes (GP), Model trees (MT), and Support Vector Machine Regression (SVMR).

\subsubsection{Results and Discussion}
Table \ref{table_rmsep_pls} shows the number of PLS components that gave the best results in terms of RMSEP. It is interesting to note that as the size of the dataset increases so too does the number of components needed to get the best results. Table \ref{table_all_methods} summarizes the findings of comparing the best PLS regression results against the other methods. As can be seen, PLS regression is only competitive on the smallest dataset. Gaussian Processes and LWPL with default parameters are clearly superior on larger datasets. The advantages gained by Gaussian Processes and LWPL in terms of RMSEP come at a cost. Model sizes for GP are quadratic in the number of samples, if predictions as well as prediction intervals are to be computed, or linear, if prediction intervals are not required; model sizes for LWPL are always linear in the number of samples. In an operational setting where hundreds of models are needed in memory or where models need frequent training, this can be a significant barrier to 
their deployment.

\begin{figure}[htb]
  \center
  \begin{tabular}{ l | r | r }
  \hline
  Dataset & Size (\# of samples) & Optimum \# PLS components \\
  \hline
  LACTIC & 255 & 6 \\
  STORIG & 414 & 39 \\
  SS & 895 & 49 \\
  OMD & 1010 & 62 \\
  DCAD & 2522 & 57 \\
  K & 6363 & 63 \\
  N & 7500 & 63 \\
  \hline
  \end{tabular}
  \caption{Comparison of RMSEP from varying the number of PLS components}
  \label{table_rmsep_pls}
\end{figure}

\begin{figure}[htb]
  \center
  \begin{tabular}{ l | l | l }
  \hline
  Dataset & Best method(s) no sig diff & Worse methods i.e. sig diff \\
  \hline
  LACTIC & PLSR(6), MT, LWPL, RRFP, SVMR & GP, LWPR \\
  STORIG & GP, LWPL & PLSR(39), MT, LWPR, RRFP, SVMR \\
  SS & GP, LWPL & PLSR(49), MT, LWPR, RRFP, SVMR \\
  OMD & GP, LWPL, LWPR, RRFP & PLSR(62), MT, SVMR \\
  DCAD & GP & PLSR(57), MT, LWPR, RRFP, SVMR, LWPL \\
  K & GP, LWPL & PLSR(63), MT, LWPR, RRFP, SVMR \\
  N & GP & PLSR(63), MT, LWPR, RRFP, SVMR, LWPL \\
  \hline
  \end{tabular}
  \caption{Comparison of all methods}
  \label{table_all_methods}
\end{figure}

\end{itemize}

%%%%%%%%%%%%%%
% Conclusion %
%%%%%%%%%%%%%%
\section{Conclusion}
ADAMS matters!

%%%%%%%%%%%%%%%%
% Bibliography %
%%%%%%%%%%%%%%%%
\begin{thebibliography}{5}

\bibitem{wagstaff2012}
Kiri L. Wagstaff (2012):
Machine Learning that Matters.
Proceedings of the Twenty-Ninth International Conference on Machine Learning (ICML), p. 529-536, 2012.

\bibitem{reutemann2012}
Peter Reutemann and Joaquin Vanschoren (2012):
Scientific Workflow Management with ADAMS.
Proceedings of the Machine Learning and Knowledge Discovery in Databases (ECML-PKDD), Part II, LNCS 7524, 2012, pp 833-837, Bristol, UK, 2012.

\bibitem{herbrich2006}
Ralf Herbrich, Xbox, Technical report TR-2006-80
http://blogs.msdn.com/b/rherb/archive/2006/06/11/626118.aspx#873165
ftp://ftp.research.microsoft.com/pub/tr/TR-2006-80.pdf

\bibitem{Dangauthier2007}
TrueSkill Through Time: Revisiting the History of Chess. Dangauthier, P.; Herbrich, R.; Minka, T.; and Graepel, T. In Advances in Neural Information Processing Systems 20 2007, page 931--938, Vancouver, 2007. The MIT Press 

\end{thebibliography}

\end{document}
